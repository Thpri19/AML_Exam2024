{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Reshape\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thor DataLoad\n",
    "# Specify the path to the manually downloaded dataset\n",
    "data_dir = r'C:\\Users\\thorp\\OneDrive\\Dokumenter\\Uni\\Kandidat\\Anvendt maskinl√¶ring\\Exam\\Data\\patch_camelyon'\n",
    "\n",
    "# Load PatchCamelyon dataset using TFDS\n",
    "def convert_sample(sample):\n",
    "    image, label = sample['image'], sample['label']  \n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    label = tf.one_hot(label, 2, dtype=tf.float32)\n",
    "    return image, image # Here i make sure that labels are not passed through since the AE does not use labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1,ds2,ds3 = tfds.load('patch_camelyon',split=['train[:20%]','test[:5%]','validation[:5%]'],\n",
    "                        data_dir = data_dir,\n",
    "                        download=False,\n",
    "                        shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset       = ds1.map(convert_sample).batch(32)\n",
    "validation_dataset  = ds3.map(convert_sample).batch(32)\n",
    "test_dataset        = ds2.map(convert_sample).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 47, 47, 32)        896       \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 23, 23, 64)        18496     \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 33856)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 135428    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 154,820\n",
      "Trainable params: 154,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(96, 96, 3))\n",
    "x = tf.keras.layers.Conv2D(32, 3, 2, activation='relu')(encoder_inputs)\n",
    "x = tf.keras.layers.Conv2D(64, 3, 2, activation='relu')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "encoded = tf.keras.layers.Dense(2 * latent_dim)(x)  # 2 for mean and standard deviation\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=encoder_inputs, outputs=encoded)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 2304)              6912      \n",
      "                                                                 \n",
      " reshape_8 (Reshape)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_30 (Conv2D  (None, 12, 12, 128)      73856     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_31 (Conv2D  (None, 24, 24, 64)       73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_32 (Conv2D  (None, 48, 48, 32)       18464     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_33 (Conv2D  (None, 96, 96, 3)        867       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,891\n",
      "Trainable params: 173,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=6*6*64, activation='relu', input_shape=(latent_dim,)),\n",
    "    tf.keras.layers.Reshape(target_shape=(6, 6, 64)),  # To get in \"image format\"\n",
    "    tf.keras.layers.Conv2DTranspose(128, 3, 2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2DTranspose(64, 3, 2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2DTranspose(32, 3, 2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2DTranspose(3, 3, 2, padding='same', activation='sigmoid'),  # Adjust stride to get to 96x96x3\n",
    "])\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def encode(self, x):\n",
    "        params = self.encoder(x)\n",
    "        return tf.split(params, num_or_size_splits=2, axis=1) # mean, logvar\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean #sigma= sqrt(exp(logvar))\n",
    "    \n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return tf.sigmoid(self.decode(eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    vals = -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n",
    "    return tf.reduce_sum(vals, axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    # Output from encoder\n",
    "    mean, logvar = model.encode(x)    \n",
    "    # The reparameterization trick\n",
    "    z = model.reparameterize(mean, logvar)    \n",
    "    # We assume that p(x|z) is multivariate Bernoulli, ie. the final dense layer \n",
    "    # has a sigmoid activation function, see page. 11\n",
    "    # in Kingma, D. P., & Welling, M. (2013).\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, \n",
    "                                                        labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])    \n",
    "    # Assume normaility of p(z)\n",
    "    logpz = log_normal_pdf(z, 0., 0.)    \n",
    "    # Assume normality of q(z|x)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    # -tf.reduce_mean(decoder + sampler - encoder)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim, encoder, decoder)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "test_sample = next(iter(test_dataset.take(1)))[:16]\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_sample):\n",
    "    mean, logvar = model.encode(test_sample)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    predictions = model.sample(z)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('./vae-img/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"model_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(32, 96, 96, 3), dtype=float32, numpy=\narray([[[[0.57254905, 0.3019608 , 0.6666667 ],\n         [0.69411767, 0.41960788, 0.76470596],\n         [0.81568635, 0.5529412 , 0.86274517],\n         ...,\n         [0.1137255 , 0.07843138, 0.39607847],\n         [0.52156866, 0.33333334, 0.73333335],\n         [0.427451  , 0.10196079, 0.5647059 ]],\n\n        [[0.6       , 0.34509805, 0.65882355],\n         [0.6784314 , 0.43529415, 0.7294118 ],\n         [0.90196085, 0.6862745 , 0.95294124],\n         ...,\n         [0.15686275, 0.1254902 , 0.45098042],\n         [0.27058825, 0.10980393, 0.50980395],\n         [0.43137258, 0.16470589, 0.60784316]],\n\n        [[0.7294118 , 0.49411768, 0.74509805],\n         [0.7960785 , 0.6       , 0.8313726 ],\n         [0.8313726 , 0.6784314 , 0.90196085],\n         ...,\n         [0.07843138, 0.03921569, 0.3803922 ],\n         [0.3921569 , 0.27450982, 0.65882355],\n         [0.40000004, 0.19607845, 0.61960787]],\n\n        ...,\n\n        [[0.16470589, 0.08627451, 0.41960788],\n         [0.227451  , 0.07450981, 0.43137258],\n         [0.38431376, 0.16470589, 0.54509807],\n         ...,\n         [0.29803923, 0.16862746, 0.5254902 ],\n         [0.32941177, 0.20392159, 0.58431375],\n         [0.31764707, 0.20000002, 0.5921569 ]],\n\n        [[0.20000002, 0.12156864, 0.50980395],\n         [0.2784314 , 0.1137255 , 0.5137255 ],\n         [0.43529415, 0.20784315, 0.6117647 ],\n         ...,\n         [0.3647059 , 0.24705884, 0.58431375],\n         [0.28627452, 0.18431373, 0.5411765 ],\n         [0.23529413, 0.13725491, 0.5058824 ]],\n\n        [[0.30980393, 0.23529413, 0.654902  ],\n         [0.29411766, 0.12156864, 0.5568628 ],\n         [0.40784317, 0.16470589, 0.5921569 ],\n         ...,\n         [0.5686275 , 0.46274513, 0.7803922 ],\n         [0.33333334, 0.24705884, 0.5803922 ],\n         [0.20392159, 0.1254902 , 0.47450984]]],\n\n\n       [[[0.8705883 , 0.70980394, 0.8431373 ],\n         [0.6745098 , 0.50980395, 0.6666667 ],\n         [0.5529412 , 0.37254903, 0.5568628 ],\n         ...,\n         [0.62352943, 0.4039216 , 0.5803922 ],\n         [0.4666667 , 0.27450982, 0.45882356],\n         [0.4784314 , 0.29803923, 0.4901961 ]],\n\n        [[0.909804  , 0.73333335, 0.8862746 ],\n         [0.48235297, 0.30588236, 0.4666667 ],\n         [0.6509804 , 0.47058827, 0.6509804 ],\n         ...,\n         [0.6313726 , 0.4156863 , 0.57254905],\n         [0.6313726 , 0.43921572, 0.6156863 ],\n         [0.5294118 , 0.34901962, 0.53333336]],\n\n        [[0.45882356, 0.26666668, 0.4431373 ],\n         [0.6117647 , 0.427451  , 0.6       ],\n         [0.5529412 , 0.36862746, 0.53333336],\n         ...,\n         [0.7490196 , 0.54509807, 0.6862745 ],\n         [0.59607846, 0.40784317, 0.5647059 ],\n         [0.5803922 , 0.40000004, 0.5803922 ]],\n\n        ...,\n\n        [[0.8588236 , 0.627451  , 0.7372549 ],\n         [0.93725497, 0.7176471 , 0.8313726 ],\n         [0.90196085, 0.6901961 , 0.8235295 ],\n         ...,\n         [0.8000001 , 0.61960787, 0.72156864],\n         [0.8352942 , 0.654902  , 0.76470596],\n         [0.83921576, 0.64705884, 0.7725491 ]],\n\n        [[0.7490196 , 0.45882356, 0.5882353 ],\n         [0.69411767, 0.41960788, 0.5529412 ],\n         [0.7607844 , 0.50980395, 0.64705884],\n         ...,\n         [0.90196085, 0.7254902 , 0.8078432 ],\n         [0.8352942 , 0.64705884, 0.7490196 ],\n         [0.8431373 , 0.6509804 , 0.76470596]],\n\n        [[0.93725497, 0.6156863 , 0.7568628 ],\n         [0.854902  , 0.5529412 , 0.69411767],\n         [0.7607844 , 0.48627454, 0.627451  ],\n         ...,\n         [0.82745105, 0.6431373 , 0.72156864],\n         [0.82745105, 0.6392157 , 0.7254902 ],\n         [0.8980393 , 0.70980394, 0.80392164]]],\n\n\n       [[[0.5529412 , 0.41960788, 0.7176471 ],\n         [0.48627454, 0.3529412 , 0.6509804 ],\n         [0.15686275, 0.07843138, 0.35686275],\n         ...,\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085]],\n\n        [[0.5568628 , 0.43921572, 0.72156864],\n         [0.36862746, 0.2509804 , 0.53333336],\n         [0.14117648, 0.07450981, 0.34901962],\n         ...,\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ]],\n\n        [[0.54901963, 0.4431373 , 0.7137255 ],\n         [0.27058825, 0.16470589, 0.43529415],\n         [0.19215688, 0.1254902 , 0.3921569 ],\n         ...,\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ]],\n\n        ...,\n\n        [[0.1764706 , 0.01176471, 0.34901962],\n         [0.34509805, 0.13725491, 0.49803925],\n         [0.37254903, 0.11764707, 0.48627454],\n         ...,\n         [0.3803922 , 0.30980393, 0.6       ],\n         [0.20784315, 0.1254902 , 0.427451  ],\n         [0.26666668, 0.16078432, 0.4784314 ]],\n\n        [[0.1254902 , 0.03921569, 0.36078432],\n         [0.21960786, 0.08627451, 0.42352945],\n         [0.3019608 , 0.10588236, 0.454902  ],\n         ...,\n         [0.24313727, 0.17254902, 0.47058827],\n         [0.2392157 , 0.14901961, 0.46274513],\n         [0.3803922 , 0.26666668, 0.59607846]],\n\n        [[0.19215688, 0.19215688, 0.48235297],\n         [0.20000002, 0.14509805, 0.44705886],\n         [0.20784315, 0.07058824, 0.3921569 ],\n         ...,\n         [0.25882354, 0.18823531, 0.48627454],\n         [0.2392157 , 0.14117648, 0.4666667 ],\n         [0.2901961 , 0.16078432, 0.5019608 ]]],\n\n\n       ...,\n\n\n       [[[0.3647059 , 0.20000002, 0.58431375],\n         [0.8000001 , 0.7176471 , 0.98823535],\n         [0.5058824 , 0.47450984, 0.6745098 ],\n         ...,\n         [0.60784316, 0.56078434, 0.80392164],\n         [0.24705884, 0.21176472, 0.47450984],\n         [0.13725491, 0.10588236, 0.3921569 ]],\n\n        [[0.19215688, 0.01176471, 0.39607847],\n         [0.7254902 , 0.6392157 , 0.8941177 ],\n         [0.6431373 , 0.60784316, 0.79215693],\n         ...,\n         [0.6313726 , 0.5647059 , 0.7686275 ],\n         [0.3019608 , 0.26666668, 0.48235297],\n         [0.16078432, 0.14901961, 0.37254903]],\n\n        [[0.25490198, 0.07058824, 0.43137258],\n         [0.49803925, 0.4039216 , 0.64705884],\n         [0.76470596, 0.72156864, 0.89019614],\n         ...,\n         [0.7254902 , 0.65882355, 0.8235295 ],\n         [0.23529413, 0.21568629, 0.37254903],\n         [0.07843138, 0.08627451, 0.2392157 ]],\n\n        ...,\n\n        [[0.3803922 , 0.3372549 , 0.5568628 ],\n         [0.47058827, 0.39607847, 0.6156863 ],\n         [0.74509805, 0.62352943, 0.854902  ],\n         ...,\n         [0.76470596, 0.627451  , 0.83921576],\n         [0.63529414, 0.47058827, 0.7176471 ],\n         [0.7137255 , 0.5176471 , 0.80392164]],\n\n        [[0.43529415, 0.34901962, 0.6313726 ],\n         [0.38431376, 0.27450982, 0.5647059 ],\n         [0.32941177, 0.18823531, 0.4784314 ],\n         ...,\n         [0.7843138 , 0.6313726 , 0.8862746 ],\n         [0.7411765 , 0.54509807, 0.8313726 ],\n         [0.7019608 , 0.47058827, 0.8000001 ]],\n\n        [[0.53333336, 0.41960788, 0.7490196 ],\n         [0.3803922 , 0.24705884, 0.58431375],\n         [0.50980395, 0.36078432, 0.69411767],\n         ...,\n         [0.47450984, 0.30980393, 0.5921569 ],\n         [0.68235296, 0.4666667 , 0.7843138 ],\n         [0.8117648 , 0.5529412 , 0.9058824 ]]],\n\n\n       [[[0.5411765 , 0.3254902 , 0.72156864],\n         [0.80392164, 0.64705884, 0.92549026],\n         [0.85098046, 0.77647066, 0.90196085],\n         ...,\n         [0.2784314 , 0.18823531, 0.5411765 ],\n         [0.25490198, 0.10196079, 0.4666667 ],\n         [0.3529412 , 0.13333334, 0.5137255 ]],\n\n        [[0.2509804 , 0.02745098, 0.43921572],\n         [0.7019608 , 0.5294118 , 0.83921576],\n         [0.9568628 , 0.85098046, 1.        ],\n         ...,\n         [0.31764707, 0.21960786, 0.5764706 ],\n         [0.3529412 , 0.19215688, 0.56078434],\n         [0.3921569 , 0.16862746, 0.54901963]],\n\n        [[0.28235295, 0.0627451 , 0.49803925],\n         [0.4666667 , 0.2784314 , 0.6156863 ],\n         [1.        , 0.8745099 , 1.        ],\n         ...,\n         [0.32156864, 0.19215688, 0.54901963],\n         [0.38431376, 0.20784315, 0.57254905],\n         [0.5176471 , 0.29411766, 0.6745098 ]],\n\n        ...,\n\n        [[0.83921576, 0.454902  , 0.7803922 ],\n         [0.93725497, 0.5647059 , 0.8862746 ],\n         [0.8588236 , 0.50980395, 0.8235295 ],\n         ...,\n         [0.54901963, 0.2509804 , 0.5019608 ],\n         [0.54901963, 0.18431373, 0.45882356],\n         [0.5921569 , 0.1764706 , 0.4666667 ]],\n\n        [[0.5568628 , 0.22352943, 0.5411765 ],\n         [0.58431375, 0.25882354, 0.57254905],\n         [0.73333335, 0.41176474, 0.73333335],\n         ...,\n         [0.7058824 , 0.3529412 , 0.6431373 ],\n         [0.6313726 , 0.22352943, 0.5176471 ],\n         [0.7176471 , 0.25882354, 0.5647059 ]],\n\n        [[0.65882355, 0.36862746, 0.6784314 ],\n         [0.7803922 , 0.48627454, 0.8078432 ],\n         [0.47450984, 0.18823531, 0.50980395],\n         ...,\n         [0.7254902 , 0.3254902 , 0.6509804 ],\n         [0.62352943, 0.18039216, 0.49411768],\n         [0.627451  , 0.13333334, 0.454902  ]]],\n\n\n       [[[0.81568635, 0.6901961 , 0.8862746 ],\n         [0.92549026, 0.77647066, 0.9607844 ],\n         [0.8862746 , 0.70980394, 0.87843144],\n         ...,\n         [0.69803923, 0.32156864, 0.7058824 ],\n         [0.8117648 , 0.39607847, 0.7803922 ],\n         [0.69803923, 0.30980393, 0.6745098 ]],\n\n        [[0.7607844 , 0.6       , 0.82745105],\n         [0.7686275 , 0.58431375, 0.8000001 ],\n         [0.86666673, 0.65882355, 0.86274517],\n         ...,\n         [0.6784314 , 0.28627452, 0.6745098 ],\n         [0.6784314 , 0.28627452, 0.6627451 ],\n         [0.6117647 , 0.25882354, 0.6117647 ]],\n\n        [[0.7254902 , 0.5019608 , 0.7803922 ],\n         [0.7490196 , 0.5137255 , 0.78823537],\n         [0.8235295 , 0.5803922 , 0.8352942 ],\n         ...,\n         [0.6509804 , 0.24313727, 0.6313726 ],\n         [0.7960785 , 0.427451  , 0.7960785 ],\n         [0.6431373 , 0.32941177, 0.67058825]],\n\n        ...,\n\n        [[0.7725491 , 0.42352945, 0.74509805],\n         [0.79215693, 0.48235297, 0.8000001 ],\n         [0.6431373 , 0.3647059 , 0.6784314 ],\n         ...,\n         [0.5764706 , 0.23137257, 0.5686275 ],\n         [0.65882355, 0.3372549 , 0.6666667 ],\n         [0.62352943, 0.32156864, 0.6431373 ]],\n\n        [[0.6627451 , 0.3372549 , 0.6431373 ],\n         [0.7490196 , 0.4666667 , 0.76470596],\n         [0.63529414, 0.38431376, 0.68235296],\n         ...,\n         [0.7372549 , 0.43529415, 0.7490196 ],\n         [0.6509804 , 0.35686275, 0.6784314 ],\n         [0.6313726 , 0.34509805, 0.6627451 ]],\n\n        [[0.8235295 , 0.5254902 , 0.81568635],\n         [0.7490196 , 0.48235297, 0.7686275 ],\n         [0.6156863 , 0.3921569 , 0.67058825],\n         ...,\n         [0.9960785 , 0.7176471 , 1.        ],\n         [0.9176471 , 0.6392157 , 0.95294124],\n         [0.77647066, 0.49803925, 0.8117648 ]]]], dtype=float32)>, <tf.Tensor: shape=(32, 96, 96, 3), dtype=float32, numpy=\narray([[[[0.57254905, 0.3019608 , 0.6666667 ],\n         [0.69411767, 0.41960788, 0.76470596],\n         [0.81568635, 0.5529412 , 0.86274517],\n         ...,\n         [0.1137255 , 0.07843138, 0.39607847],\n         [0.52156866, 0.33333334, 0.73333335],\n         [0.427451  , 0.10196079, 0.5647059 ]],\n\n        [[0.6       , 0.34509805, 0.65882355],\n         [0.6784314 , 0.43529415, 0.7294118 ],\n         [0.90196085, 0.6862745 , 0.95294124],\n         ...,\n         [0.15686275, 0.1254902 , 0.45098042],\n         [0.27058825, 0.10980393, 0.50980395],\n         [0.43137258, 0.16470589, 0.60784316]],\n\n        [[0.7294118 , 0.49411768, 0.74509805],\n         [0.7960785 , 0.6       , 0.8313726 ],\n         [0.8313726 , 0.6784314 , 0.90196085],\n         ...,\n         [0.07843138, 0.03921569, 0.3803922 ],\n         [0.3921569 , 0.27450982, 0.65882355],\n         [0.40000004, 0.19607845, 0.61960787]],\n\n        ...,\n\n        [[0.16470589, 0.08627451, 0.41960788],\n         [0.227451  , 0.07450981, 0.43137258],\n         [0.38431376, 0.16470589, 0.54509807],\n         ...,\n         [0.29803923, 0.16862746, 0.5254902 ],\n         [0.32941177, 0.20392159, 0.58431375],\n         [0.31764707, 0.20000002, 0.5921569 ]],\n\n        [[0.20000002, 0.12156864, 0.50980395],\n         [0.2784314 , 0.1137255 , 0.5137255 ],\n         [0.43529415, 0.20784315, 0.6117647 ],\n         ...,\n         [0.3647059 , 0.24705884, 0.58431375],\n         [0.28627452, 0.18431373, 0.5411765 ],\n         [0.23529413, 0.13725491, 0.5058824 ]],\n\n        [[0.30980393, 0.23529413, 0.654902  ],\n         [0.29411766, 0.12156864, 0.5568628 ],\n         [0.40784317, 0.16470589, 0.5921569 ],\n         ...,\n         [0.5686275 , 0.46274513, 0.7803922 ],\n         [0.33333334, 0.24705884, 0.5803922 ],\n         [0.20392159, 0.1254902 , 0.47450984]]],\n\n\n       [[[0.8705883 , 0.70980394, 0.8431373 ],\n         [0.6745098 , 0.50980395, 0.6666667 ],\n         [0.5529412 , 0.37254903, 0.5568628 ],\n         ...,\n         [0.62352943, 0.4039216 , 0.5803922 ],\n         [0.4666667 , 0.27450982, 0.45882356],\n         [0.4784314 , 0.29803923, 0.4901961 ]],\n\n        [[0.909804  , 0.73333335, 0.8862746 ],\n         [0.48235297, 0.30588236, 0.4666667 ],\n         [0.6509804 , 0.47058827, 0.6509804 ],\n         ...,\n         [0.6313726 , 0.4156863 , 0.57254905],\n         [0.6313726 , 0.43921572, 0.6156863 ],\n         [0.5294118 , 0.34901962, 0.53333336]],\n\n        [[0.45882356, 0.26666668, 0.4431373 ],\n         [0.6117647 , 0.427451  , 0.6       ],\n         [0.5529412 , 0.36862746, 0.53333336],\n         ...,\n         [0.7490196 , 0.54509807, 0.6862745 ],\n         [0.59607846, 0.40784317, 0.5647059 ],\n         [0.5803922 , 0.40000004, 0.5803922 ]],\n\n        ...,\n\n        [[0.8588236 , 0.627451  , 0.7372549 ],\n         [0.93725497, 0.7176471 , 0.8313726 ],\n         [0.90196085, 0.6901961 , 0.8235295 ],\n         ...,\n         [0.8000001 , 0.61960787, 0.72156864],\n         [0.8352942 , 0.654902  , 0.76470596],\n         [0.83921576, 0.64705884, 0.7725491 ]],\n\n        [[0.7490196 , 0.45882356, 0.5882353 ],\n         [0.69411767, 0.41960788, 0.5529412 ],\n         [0.7607844 , 0.50980395, 0.64705884],\n         ...,\n         [0.90196085, 0.7254902 , 0.8078432 ],\n         [0.8352942 , 0.64705884, 0.7490196 ],\n         [0.8431373 , 0.6509804 , 0.76470596]],\n\n        [[0.93725497, 0.6156863 , 0.7568628 ],\n         [0.854902  , 0.5529412 , 0.69411767],\n         [0.7607844 , 0.48627454, 0.627451  ],\n         ...,\n         [0.82745105, 0.6431373 , 0.72156864],\n         [0.82745105, 0.6392157 , 0.7254902 ],\n         [0.8980393 , 0.70980394, 0.80392164]]],\n\n\n       [[[0.5529412 , 0.41960788, 0.7176471 ],\n         [0.48627454, 0.3529412 , 0.6509804 ],\n         [0.15686275, 0.07843138, 0.35686275],\n         ...,\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085]],\n\n        [[0.5568628 , 0.43921572, 0.72156864],\n         [0.36862746, 0.2509804 , 0.53333336],\n         [0.14117648, 0.07450981, 0.34901962],\n         ...,\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ]],\n\n        [[0.54901963, 0.4431373 , 0.7137255 ],\n         [0.27058825, 0.16470589, 0.43529415],\n         [0.19215688, 0.1254902 , 0.3921569 ],\n         ...,\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ]],\n\n        ...,\n\n        [[0.1764706 , 0.01176471, 0.34901962],\n         [0.34509805, 0.13725491, 0.49803925],\n         [0.37254903, 0.11764707, 0.48627454],\n         ...,\n         [0.3803922 , 0.30980393, 0.6       ],\n         [0.20784315, 0.1254902 , 0.427451  ],\n         [0.26666668, 0.16078432, 0.4784314 ]],\n\n        [[0.1254902 , 0.03921569, 0.36078432],\n         [0.21960786, 0.08627451, 0.42352945],\n         [0.3019608 , 0.10588236, 0.454902  ],\n         ...,\n         [0.24313727, 0.17254902, 0.47058827],\n         [0.2392157 , 0.14901961, 0.46274513],\n         [0.3803922 , 0.26666668, 0.59607846]],\n\n        [[0.19215688, 0.19215688, 0.48235297],\n         [0.20000002, 0.14509805, 0.44705886],\n         [0.20784315, 0.07058824, 0.3921569 ],\n         ...,\n         [0.25882354, 0.18823531, 0.48627454],\n         [0.2392157 , 0.14117648, 0.4666667 ],\n         [0.2901961 , 0.16078432, 0.5019608 ]]],\n\n\n       ...,\n\n\n       [[[0.3647059 , 0.20000002, 0.58431375],\n         [0.8000001 , 0.7176471 , 0.98823535],\n         [0.5058824 , 0.47450984, 0.6745098 ],\n         ...,\n         [0.60784316, 0.56078434, 0.80392164],\n         [0.24705884, 0.21176472, 0.47450984],\n         [0.13725491, 0.10588236, 0.3921569 ]],\n\n        [[0.19215688, 0.01176471, 0.39607847],\n         [0.7254902 , 0.6392157 , 0.8941177 ],\n         [0.6431373 , 0.60784316, 0.79215693],\n         ...,\n         [0.6313726 , 0.5647059 , 0.7686275 ],\n         [0.3019608 , 0.26666668, 0.48235297],\n         [0.16078432, 0.14901961, 0.37254903]],\n\n        [[0.25490198, 0.07058824, 0.43137258],\n         [0.49803925, 0.4039216 , 0.64705884],\n         [0.76470596, 0.72156864, 0.89019614],\n         ...,\n         [0.7254902 , 0.65882355, 0.8235295 ],\n         [0.23529413, 0.21568629, 0.37254903],\n         [0.07843138, 0.08627451, 0.2392157 ]],\n\n        ...,\n\n        [[0.3803922 , 0.3372549 , 0.5568628 ],\n         [0.47058827, 0.39607847, 0.6156863 ],\n         [0.74509805, 0.62352943, 0.854902  ],\n         ...,\n         [0.76470596, 0.627451  , 0.83921576],\n         [0.63529414, 0.47058827, 0.7176471 ],\n         [0.7137255 , 0.5176471 , 0.80392164]],\n\n        [[0.43529415, 0.34901962, 0.6313726 ],\n         [0.38431376, 0.27450982, 0.5647059 ],\n         [0.32941177, 0.18823531, 0.4784314 ],\n         ...,\n         [0.7843138 , 0.6313726 , 0.8862746 ],\n         [0.7411765 , 0.54509807, 0.8313726 ],\n         [0.7019608 , 0.47058827, 0.8000001 ]],\n\n        [[0.53333336, 0.41960788, 0.7490196 ],\n         [0.3803922 , 0.24705884, 0.58431375],\n         [0.50980395, 0.36078432, 0.69411767],\n         ...,\n         [0.47450984, 0.30980393, 0.5921569 ],\n         [0.68235296, 0.4666667 , 0.7843138 ],\n         [0.8117648 , 0.5529412 , 0.9058824 ]]],\n\n\n       [[[0.5411765 , 0.3254902 , 0.72156864],\n         [0.80392164, 0.64705884, 0.92549026],\n         [0.85098046, 0.77647066, 0.90196085],\n         ...,\n         [0.2784314 , 0.18823531, 0.5411765 ],\n         [0.25490198, 0.10196079, 0.4666667 ],\n         [0.3529412 , 0.13333334, 0.5137255 ]],\n\n        [[0.2509804 , 0.02745098, 0.43921572],\n         [0.7019608 , 0.5294118 , 0.83921576],\n         [0.9568628 , 0.85098046, 1.        ],\n         ...,\n         [0.31764707, 0.21960786, 0.5764706 ],\n         [0.3529412 , 0.19215688, 0.56078434],\n         [0.3921569 , 0.16862746, 0.54901963]],\n\n        [[0.28235295, 0.0627451 , 0.49803925],\n         [0.4666667 , 0.2784314 , 0.6156863 ],\n         [1.        , 0.8745099 , 1.        ],\n         ...,\n         [0.32156864, 0.19215688, 0.54901963],\n         [0.38431376, 0.20784315, 0.57254905],\n         [0.5176471 , 0.29411766, 0.6745098 ]],\n\n        ...,\n\n        [[0.83921576, 0.454902  , 0.7803922 ],\n         [0.93725497, 0.5647059 , 0.8862746 ],\n         [0.8588236 , 0.50980395, 0.8235295 ],\n         ...,\n         [0.54901963, 0.2509804 , 0.5019608 ],\n         [0.54901963, 0.18431373, 0.45882356],\n         [0.5921569 , 0.1764706 , 0.4666667 ]],\n\n        [[0.5568628 , 0.22352943, 0.5411765 ],\n         [0.58431375, 0.25882354, 0.57254905],\n         [0.73333335, 0.41176474, 0.73333335],\n         ...,\n         [0.7058824 , 0.3529412 , 0.6431373 ],\n         [0.6313726 , 0.22352943, 0.5176471 ],\n         [0.7176471 , 0.25882354, 0.5647059 ]],\n\n        [[0.65882355, 0.36862746, 0.6784314 ],\n         [0.7803922 , 0.48627454, 0.8078432 ],\n         [0.47450984, 0.18823531, 0.50980395],\n         ...,\n         [0.7254902 , 0.3254902 , 0.6509804 ],\n         [0.62352943, 0.18039216, 0.49411768],\n         [0.627451  , 0.13333334, 0.454902  ]]],\n\n\n       [[[0.81568635, 0.6901961 , 0.8862746 ],\n         [0.92549026, 0.77647066, 0.9607844 ],\n         [0.8862746 , 0.70980394, 0.87843144],\n         ...,\n         [0.69803923, 0.32156864, 0.7058824 ],\n         [0.8117648 , 0.39607847, 0.7803922 ],\n         [0.69803923, 0.30980393, 0.6745098 ]],\n\n        [[0.7607844 , 0.6       , 0.82745105],\n         [0.7686275 , 0.58431375, 0.8000001 ],\n         [0.86666673, 0.65882355, 0.86274517],\n         ...,\n         [0.6784314 , 0.28627452, 0.6745098 ],\n         [0.6784314 , 0.28627452, 0.6627451 ],\n         [0.6117647 , 0.25882354, 0.6117647 ]],\n\n        [[0.7254902 , 0.5019608 , 0.7803922 ],\n         [0.7490196 , 0.5137255 , 0.78823537],\n         [0.8235295 , 0.5803922 , 0.8352942 ],\n         ...,\n         [0.6509804 , 0.24313727, 0.6313726 ],\n         [0.7960785 , 0.427451  , 0.7960785 ],\n         [0.6431373 , 0.32941177, 0.67058825]],\n\n        ...,\n\n        [[0.7725491 , 0.42352945, 0.74509805],\n         [0.79215693, 0.48235297, 0.8000001 ],\n         [0.6431373 , 0.3647059 , 0.6784314 ],\n         ...,\n         [0.5764706 , 0.23137257, 0.5686275 ],\n         [0.65882355, 0.3372549 , 0.6666667 ],\n         [0.62352943, 0.32156864, 0.6431373 ]],\n\n        [[0.6627451 , 0.3372549 , 0.6431373 ],\n         [0.7490196 , 0.4666667 , 0.76470596],\n         [0.63529414, 0.38431376, 0.68235296],\n         ...,\n         [0.7372549 , 0.43529415, 0.7490196 ],\n         [0.6509804 , 0.35686275, 0.6784314 ],\n         [0.6313726 , 0.34509805, 0.6627451 ]],\n\n        [[0.8235295 , 0.5254902 , 0.81568635],\n         [0.7490196 , 0.48235297, 0.7686275 ],\n         [0.6156863 , 0.3921569 , 0.67058825],\n         ...,\n         [0.9960785 , 0.7176471 , 1.        ],\n         [0.9176471 , 0.6392157 , 0.95294124],\n         [0.77647066, 0.49803925, 0.8117648 ]]]], dtype=float32)>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m  loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMean()\n\u001b[0;32m      6\u001b[0m  \u001b[38;5;28;01mfor\u001b[39;00m test_x \u001b[38;5;129;01min\u001b[39;00m test_dataset:\n\u001b[1;32m----> 7\u001b[0m      loss(\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m  variational_lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mloss\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m     10\u001b[0m  \u001b[38;5;66;03m#print(f'Epoch: {epoch}, Test set variational lower bound:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#        {variational_lower_bound}')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 8\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(model, x):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Output from encoder\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# The reparameterization trick\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreparameterize(mean, logvar)    \n",
      "Cell \u001b[1;32mIn[62], line 9\u001b[0m, in \u001b[0;36mVAE.encode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msplit(params, num_or_size_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thorp\\.conda\\envs\\amfall23Copy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\thorp\\.conda\\envs\\amfall23Copy\\lib\\site-packages\\keras\\engine\\input_spec.py:216\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs to a layer should be tensors. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     )\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"model_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor: shape=(32, 96, 96, 3), dtype=float32, numpy=\narray([[[[0.57254905, 0.3019608 , 0.6666667 ],\n         [0.69411767, 0.41960788, 0.76470596],\n         [0.81568635, 0.5529412 , 0.86274517],\n         ...,\n         [0.1137255 , 0.07843138, 0.39607847],\n         [0.52156866, 0.33333334, 0.73333335],\n         [0.427451  , 0.10196079, 0.5647059 ]],\n\n        [[0.6       , 0.34509805, 0.65882355],\n         [0.6784314 , 0.43529415, 0.7294118 ],\n         [0.90196085, 0.6862745 , 0.95294124],\n         ...,\n         [0.15686275, 0.1254902 , 0.45098042],\n         [0.27058825, 0.10980393, 0.50980395],\n         [0.43137258, 0.16470589, 0.60784316]],\n\n        [[0.7294118 , 0.49411768, 0.74509805],\n         [0.7960785 , 0.6       , 0.8313726 ],\n         [0.8313726 , 0.6784314 , 0.90196085],\n         ...,\n         [0.07843138, 0.03921569, 0.3803922 ],\n         [0.3921569 , 0.27450982, 0.65882355],\n         [0.40000004, 0.19607845, 0.61960787]],\n\n        ...,\n\n        [[0.16470589, 0.08627451, 0.41960788],\n         [0.227451  , 0.07450981, 0.43137258],\n         [0.38431376, 0.16470589, 0.54509807],\n         ...,\n         [0.29803923, 0.16862746, 0.5254902 ],\n         [0.32941177, 0.20392159, 0.58431375],\n         [0.31764707, 0.20000002, 0.5921569 ]],\n\n        [[0.20000002, 0.12156864, 0.50980395],\n         [0.2784314 , 0.1137255 , 0.5137255 ],\n         [0.43529415, 0.20784315, 0.6117647 ],\n         ...,\n         [0.3647059 , 0.24705884, 0.58431375],\n         [0.28627452, 0.18431373, 0.5411765 ],\n         [0.23529413, 0.13725491, 0.5058824 ]],\n\n        [[0.30980393, 0.23529413, 0.654902  ],\n         [0.29411766, 0.12156864, 0.5568628 ],\n         [0.40784317, 0.16470589, 0.5921569 ],\n         ...,\n         [0.5686275 , 0.46274513, 0.7803922 ],\n         [0.33333334, 0.24705884, 0.5803922 ],\n         [0.20392159, 0.1254902 , 0.47450984]]],\n\n\n       [[[0.8705883 , 0.70980394, 0.8431373 ],\n         [0.6745098 , 0.50980395, 0.6666667 ],\n         [0.5529412 , 0.37254903, 0.5568628 ],\n         ...,\n         [0.62352943, 0.4039216 , 0.5803922 ],\n         [0.4666667 , 0.27450982, 0.45882356],\n         [0.4784314 , 0.29803923, 0.4901961 ]],\n\n        [[0.909804  , 0.73333335, 0.8862746 ],\n         [0.48235297, 0.30588236, 0.4666667 ],\n         [0.6509804 , 0.47058827, 0.6509804 ],\n         ...,\n         [0.6313726 , 0.4156863 , 0.57254905],\n         [0.6313726 , 0.43921572, 0.6156863 ],\n         [0.5294118 , 0.34901962, 0.53333336]],\n\n        [[0.45882356, 0.26666668, 0.4431373 ],\n         [0.6117647 , 0.427451  , 0.6       ],\n         [0.5529412 , 0.36862746, 0.53333336],\n         ...,\n         [0.7490196 , 0.54509807, 0.6862745 ],\n         [0.59607846, 0.40784317, 0.5647059 ],\n         [0.5803922 , 0.40000004, 0.5803922 ]],\n\n        ...,\n\n        [[0.8588236 , 0.627451  , 0.7372549 ],\n         [0.93725497, 0.7176471 , 0.8313726 ],\n         [0.90196085, 0.6901961 , 0.8235295 ],\n         ...,\n         [0.8000001 , 0.61960787, 0.72156864],\n         [0.8352942 , 0.654902  , 0.76470596],\n         [0.83921576, 0.64705884, 0.7725491 ]],\n\n        [[0.7490196 , 0.45882356, 0.5882353 ],\n         [0.69411767, 0.41960788, 0.5529412 ],\n         [0.7607844 , 0.50980395, 0.64705884],\n         ...,\n         [0.90196085, 0.7254902 , 0.8078432 ],\n         [0.8352942 , 0.64705884, 0.7490196 ],\n         [0.8431373 , 0.6509804 , 0.76470596]],\n\n        [[0.93725497, 0.6156863 , 0.7568628 ],\n         [0.854902  , 0.5529412 , 0.69411767],\n         [0.7607844 , 0.48627454, 0.627451  ],\n         ...,\n         [0.82745105, 0.6431373 , 0.72156864],\n         [0.82745105, 0.6392157 , 0.7254902 ],\n         [0.8980393 , 0.70980394, 0.80392164]]],\n\n\n       [[[0.5529412 , 0.41960788, 0.7176471 ],\n         [0.48627454, 0.3529412 , 0.6509804 ],\n         [0.15686275, 0.07843138, 0.35686275],\n         ...,\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085]],\n\n        [[0.5568628 , 0.43921572, 0.72156864],\n         [0.36862746, 0.2509804 , 0.53333336],\n         [0.14117648, 0.07450981, 0.34901962],\n         ...,\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ]],\n\n        [[0.54901963, 0.4431373 , 0.7137255 ],\n         [0.27058825, 0.16470589, 0.43529415],\n         [0.19215688, 0.1254902 , 0.3921569 ],\n         ...,\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ]],\n\n        ...,\n\n        [[0.1764706 , 0.01176471, 0.34901962],\n         [0.34509805, 0.13725491, 0.49803925],\n         [0.37254903, 0.11764707, 0.48627454],\n         ...,\n         [0.3803922 , 0.30980393, 0.6       ],\n         [0.20784315, 0.1254902 , 0.427451  ],\n         [0.26666668, 0.16078432, 0.4784314 ]],\n\n        [[0.1254902 , 0.03921569, 0.36078432],\n         [0.21960786, 0.08627451, 0.42352945],\n         [0.3019608 , 0.10588236, 0.454902  ],\n         ...,\n         [0.24313727, 0.17254902, 0.47058827],\n         [0.2392157 , 0.14901961, 0.46274513],\n         [0.3803922 , 0.26666668, 0.59607846]],\n\n        [[0.19215688, 0.19215688, 0.48235297],\n         [0.20000002, 0.14509805, 0.44705886],\n         [0.20784315, 0.07058824, 0.3921569 ],\n         ...,\n         [0.25882354, 0.18823531, 0.48627454],\n         [0.2392157 , 0.14117648, 0.4666667 ],\n         [0.2901961 , 0.16078432, 0.5019608 ]]],\n\n\n       ...,\n\n\n       [[[0.3647059 , 0.20000002, 0.58431375],\n         [0.8000001 , 0.7176471 , 0.98823535],\n         [0.5058824 , 0.47450984, 0.6745098 ],\n         ...,\n         [0.60784316, 0.56078434, 0.80392164],\n         [0.24705884, 0.21176472, 0.47450984],\n         [0.13725491, 0.10588236, 0.3921569 ]],\n\n        [[0.19215688, 0.01176471, 0.39607847],\n         [0.7254902 , 0.6392157 , 0.8941177 ],\n         [0.6431373 , 0.60784316, 0.79215693],\n         ...,\n         [0.6313726 , 0.5647059 , 0.7686275 ],\n         [0.3019608 , 0.26666668, 0.48235297],\n         [0.16078432, 0.14901961, 0.37254903]],\n\n        [[0.25490198, 0.07058824, 0.43137258],\n         [0.49803925, 0.4039216 , 0.64705884],\n         [0.76470596, 0.72156864, 0.89019614],\n         ...,\n         [0.7254902 , 0.65882355, 0.8235295 ],\n         [0.23529413, 0.21568629, 0.37254903],\n         [0.07843138, 0.08627451, 0.2392157 ]],\n\n        ...,\n\n        [[0.3803922 , 0.3372549 , 0.5568628 ],\n         [0.47058827, 0.39607847, 0.6156863 ],\n         [0.74509805, 0.62352943, 0.854902  ],\n         ...,\n         [0.76470596, 0.627451  , 0.83921576],\n         [0.63529414, 0.47058827, 0.7176471 ],\n         [0.7137255 , 0.5176471 , 0.80392164]],\n\n        [[0.43529415, 0.34901962, 0.6313726 ],\n         [0.38431376, 0.27450982, 0.5647059 ],\n         [0.32941177, 0.18823531, 0.4784314 ],\n         ...,\n         [0.7843138 , 0.6313726 , 0.8862746 ],\n         [0.7411765 , 0.54509807, 0.8313726 ],\n         [0.7019608 , 0.47058827, 0.8000001 ]],\n\n        [[0.53333336, 0.41960788, 0.7490196 ],\n         [0.3803922 , 0.24705884, 0.58431375],\n         [0.50980395, 0.36078432, 0.69411767],\n         ...,\n         [0.47450984, 0.30980393, 0.5921569 ],\n         [0.68235296, 0.4666667 , 0.7843138 ],\n         [0.8117648 , 0.5529412 , 0.9058824 ]]],\n\n\n       [[[0.5411765 , 0.3254902 , 0.72156864],\n         [0.80392164, 0.64705884, 0.92549026],\n         [0.85098046, 0.77647066, 0.90196085],\n         ...,\n         [0.2784314 , 0.18823531, 0.5411765 ],\n         [0.25490198, 0.10196079, 0.4666667 ],\n         [0.3529412 , 0.13333334, 0.5137255 ]],\n\n        [[0.2509804 , 0.02745098, 0.43921572],\n         [0.7019608 , 0.5294118 , 0.83921576],\n         [0.9568628 , 0.85098046, 1.        ],\n         ...,\n         [0.31764707, 0.21960786, 0.5764706 ],\n         [0.3529412 , 0.19215688, 0.56078434],\n         [0.3921569 , 0.16862746, 0.54901963]],\n\n        [[0.28235295, 0.0627451 , 0.49803925],\n         [0.4666667 , 0.2784314 , 0.6156863 ],\n         [1.        , 0.8745099 , 1.        ],\n         ...,\n         [0.32156864, 0.19215688, 0.54901963],\n         [0.38431376, 0.20784315, 0.57254905],\n         [0.5176471 , 0.29411766, 0.6745098 ]],\n\n        ...,\n\n        [[0.83921576, 0.454902  , 0.7803922 ],\n         [0.93725497, 0.5647059 , 0.8862746 ],\n         [0.8588236 , 0.50980395, 0.8235295 ],\n         ...,\n         [0.54901963, 0.2509804 , 0.5019608 ],\n         [0.54901963, 0.18431373, 0.45882356],\n         [0.5921569 , 0.1764706 , 0.4666667 ]],\n\n        [[0.5568628 , 0.22352943, 0.5411765 ],\n         [0.58431375, 0.25882354, 0.57254905],\n         [0.73333335, 0.41176474, 0.73333335],\n         ...,\n         [0.7058824 , 0.3529412 , 0.6431373 ],\n         [0.6313726 , 0.22352943, 0.5176471 ],\n         [0.7176471 , 0.25882354, 0.5647059 ]],\n\n        [[0.65882355, 0.36862746, 0.6784314 ],\n         [0.7803922 , 0.48627454, 0.8078432 ],\n         [0.47450984, 0.18823531, 0.50980395],\n         ...,\n         [0.7254902 , 0.3254902 , 0.6509804 ],\n         [0.62352943, 0.18039216, 0.49411768],\n         [0.627451  , 0.13333334, 0.454902  ]]],\n\n\n       [[[0.81568635, 0.6901961 , 0.8862746 ],\n         [0.92549026, 0.77647066, 0.9607844 ],\n         [0.8862746 , 0.70980394, 0.87843144],\n         ...,\n         [0.69803923, 0.32156864, 0.7058824 ],\n         [0.8117648 , 0.39607847, 0.7803922 ],\n         [0.69803923, 0.30980393, 0.6745098 ]],\n\n        [[0.7607844 , 0.6       , 0.82745105],\n         [0.7686275 , 0.58431375, 0.8000001 ],\n         [0.86666673, 0.65882355, 0.86274517],\n         ...,\n         [0.6784314 , 0.28627452, 0.6745098 ],\n         [0.6784314 , 0.28627452, 0.6627451 ],\n         [0.6117647 , 0.25882354, 0.6117647 ]],\n\n        [[0.7254902 , 0.5019608 , 0.7803922 ],\n         [0.7490196 , 0.5137255 , 0.78823537],\n         [0.8235295 , 0.5803922 , 0.8352942 ],\n         ...,\n         [0.6509804 , 0.24313727, 0.6313726 ],\n         [0.7960785 , 0.427451  , 0.7960785 ],\n         [0.6431373 , 0.32941177, 0.67058825]],\n\n        ...,\n\n        [[0.7725491 , 0.42352945, 0.74509805],\n         [0.79215693, 0.48235297, 0.8000001 ],\n         [0.6431373 , 0.3647059 , 0.6784314 ],\n         ...,\n         [0.5764706 , 0.23137257, 0.5686275 ],\n         [0.65882355, 0.3372549 , 0.6666667 ],\n         [0.62352943, 0.32156864, 0.6431373 ]],\n\n        [[0.6627451 , 0.3372549 , 0.6431373 ],\n         [0.7490196 , 0.4666667 , 0.76470596],\n         [0.63529414, 0.38431376, 0.68235296],\n         ...,\n         [0.7372549 , 0.43529415, 0.7490196 ],\n         [0.6509804 , 0.35686275, 0.6784314 ],\n         [0.6313726 , 0.34509805, 0.6627451 ]],\n\n        [[0.8235295 , 0.5254902 , 0.81568635],\n         [0.7490196 , 0.48235297, 0.7686275 ],\n         [0.6156863 , 0.3921569 , 0.67058825],\n         ...,\n         [0.9960785 , 0.7176471 , 1.        ],\n         [0.9176471 , 0.6392157 , 0.95294124],\n         [0.77647066, 0.49803925, 0.8117648 ]]]], dtype=float32)>, <tf.Tensor: shape=(32, 96, 96, 3), dtype=float32, numpy=\narray([[[[0.57254905, 0.3019608 , 0.6666667 ],\n         [0.69411767, 0.41960788, 0.76470596],\n         [0.81568635, 0.5529412 , 0.86274517],\n         ...,\n         [0.1137255 , 0.07843138, 0.39607847],\n         [0.52156866, 0.33333334, 0.73333335],\n         [0.427451  , 0.10196079, 0.5647059 ]],\n\n        [[0.6       , 0.34509805, 0.65882355],\n         [0.6784314 , 0.43529415, 0.7294118 ],\n         [0.90196085, 0.6862745 , 0.95294124],\n         ...,\n         [0.15686275, 0.1254902 , 0.45098042],\n         [0.27058825, 0.10980393, 0.50980395],\n         [0.43137258, 0.16470589, 0.60784316]],\n\n        [[0.7294118 , 0.49411768, 0.74509805],\n         [0.7960785 , 0.6       , 0.8313726 ],\n         [0.8313726 , 0.6784314 , 0.90196085],\n         ...,\n         [0.07843138, 0.03921569, 0.3803922 ],\n         [0.3921569 , 0.27450982, 0.65882355],\n         [0.40000004, 0.19607845, 0.61960787]],\n\n        ...,\n\n        [[0.16470589, 0.08627451, 0.41960788],\n         [0.227451  , 0.07450981, 0.43137258],\n         [0.38431376, 0.16470589, 0.54509807],\n         ...,\n         [0.29803923, 0.16862746, 0.5254902 ],\n         [0.32941177, 0.20392159, 0.58431375],\n         [0.31764707, 0.20000002, 0.5921569 ]],\n\n        [[0.20000002, 0.12156864, 0.50980395],\n         [0.2784314 , 0.1137255 , 0.5137255 ],\n         [0.43529415, 0.20784315, 0.6117647 ],\n         ...,\n         [0.3647059 , 0.24705884, 0.58431375],\n         [0.28627452, 0.18431373, 0.5411765 ],\n         [0.23529413, 0.13725491, 0.5058824 ]],\n\n        [[0.30980393, 0.23529413, 0.654902  ],\n         [0.29411766, 0.12156864, 0.5568628 ],\n         [0.40784317, 0.16470589, 0.5921569 ],\n         ...,\n         [0.5686275 , 0.46274513, 0.7803922 ],\n         [0.33333334, 0.24705884, 0.5803922 ],\n         [0.20392159, 0.1254902 , 0.47450984]]],\n\n\n       [[[0.8705883 , 0.70980394, 0.8431373 ],\n         [0.6745098 , 0.50980395, 0.6666667 ],\n         [0.5529412 , 0.37254903, 0.5568628 ],\n         ...,\n         [0.62352943, 0.4039216 , 0.5803922 ],\n         [0.4666667 , 0.27450982, 0.45882356],\n         [0.4784314 , 0.29803923, 0.4901961 ]],\n\n        [[0.909804  , 0.73333335, 0.8862746 ],\n         [0.48235297, 0.30588236, 0.4666667 ],\n         [0.6509804 , 0.47058827, 0.6509804 ],\n         ...,\n         [0.6313726 , 0.4156863 , 0.57254905],\n         [0.6313726 , 0.43921572, 0.6156863 ],\n         [0.5294118 , 0.34901962, 0.53333336]],\n\n        [[0.45882356, 0.26666668, 0.4431373 ],\n         [0.6117647 , 0.427451  , 0.6       ],\n         [0.5529412 , 0.36862746, 0.53333336],\n         ...,\n         [0.7490196 , 0.54509807, 0.6862745 ],\n         [0.59607846, 0.40784317, 0.5647059 ],\n         [0.5803922 , 0.40000004, 0.5803922 ]],\n\n        ...,\n\n        [[0.8588236 , 0.627451  , 0.7372549 ],\n         [0.93725497, 0.7176471 , 0.8313726 ],\n         [0.90196085, 0.6901961 , 0.8235295 ],\n         ...,\n         [0.8000001 , 0.61960787, 0.72156864],\n         [0.8352942 , 0.654902  , 0.76470596],\n         [0.83921576, 0.64705884, 0.7725491 ]],\n\n        [[0.7490196 , 0.45882356, 0.5882353 ],\n         [0.69411767, 0.41960788, 0.5529412 ],\n         [0.7607844 , 0.50980395, 0.64705884],\n         ...,\n         [0.90196085, 0.7254902 , 0.8078432 ],\n         [0.8352942 , 0.64705884, 0.7490196 ],\n         [0.8431373 , 0.6509804 , 0.76470596]],\n\n        [[0.93725497, 0.6156863 , 0.7568628 ],\n         [0.854902  , 0.5529412 , 0.69411767],\n         [0.7607844 , 0.48627454, 0.627451  ],\n         ...,\n         [0.82745105, 0.6431373 , 0.72156864],\n         [0.82745105, 0.6392157 , 0.7254902 ],\n         [0.8980393 , 0.70980394, 0.80392164]]],\n\n\n       [[[0.5529412 , 0.41960788, 0.7176471 ],\n         [0.48627454, 0.3529412 , 0.6509804 ],\n         [0.15686275, 0.07843138, 0.35686275],\n         ...,\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085],\n         [0.8862746 , 0.86274517, 0.90196085]],\n\n        [[0.5568628 , 0.43921572, 0.72156864],\n         [0.36862746, 0.2509804 , 0.53333336],\n         [0.14117648, 0.07450981, 0.34901962],\n         ...,\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ],\n         [0.882353  , 0.8588236 , 0.8980393 ]],\n\n        [[0.54901963, 0.4431373 , 0.7137255 ],\n         [0.27058825, 0.16470589, 0.43529415],\n         [0.19215688, 0.1254902 , 0.3921569 ],\n         ...,\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ],\n         [0.87843144, 0.854902  , 0.8941177 ]],\n\n        ...,\n\n        [[0.1764706 , 0.01176471, 0.34901962],\n         [0.34509805, 0.13725491, 0.49803925],\n         [0.37254903, 0.11764707, 0.48627454],\n         ...,\n         [0.3803922 , 0.30980393, 0.6       ],\n         [0.20784315, 0.1254902 , 0.427451  ],\n         [0.26666668, 0.16078432, 0.4784314 ]],\n\n        [[0.1254902 , 0.03921569, 0.36078432],\n         [0.21960786, 0.08627451, 0.42352945],\n         [0.3019608 , 0.10588236, 0.454902  ],\n         ...,\n         [0.24313727, 0.17254902, 0.47058827],\n         [0.2392157 , 0.14901961, 0.46274513],\n         [0.3803922 , 0.26666668, 0.59607846]],\n\n        [[0.19215688, 0.19215688, 0.48235297],\n         [0.20000002, 0.14509805, 0.44705886],\n         [0.20784315, 0.07058824, 0.3921569 ],\n         ...,\n         [0.25882354, 0.18823531, 0.48627454],\n         [0.2392157 , 0.14117648, 0.4666667 ],\n         [0.2901961 , 0.16078432, 0.5019608 ]]],\n\n\n       ...,\n\n\n       [[[0.3647059 , 0.20000002, 0.58431375],\n         [0.8000001 , 0.7176471 , 0.98823535],\n         [0.5058824 , 0.47450984, 0.6745098 ],\n         ...,\n         [0.60784316, 0.56078434, 0.80392164],\n         [0.24705884, 0.21176472, 0.47450984],\n         [0.13725491, 0.10588236, 0.3921569 ]],\n\n        [[0.19215688, 0.01176471, 0.39607847],\n         [0.7254902 , 0.6392157 , 0.8941177 ],\n         [0.6431373 , 0.60784316, 0.79215693],\n         ...,\n         [0.6313726 , 0.5647059 , 0.7686275 ],\n         [0.3019608 , 0.26666668, 0.48235297],\n         [0.16078432, 0.14901961, 0.37254903]],\n\n        [[0.25490198, 0.07058824, 0.43137258],\n         [0.49803925, 0.4039216 , 0.64705884],\n         [0.76470596, 0.72156864, 0.89019614],\n         ...,\n         [0.7254902 , 0.65882355, 0.8235295 ],\n         [0.23529413, 0.21568629, 0.37254903],\n         [0.07843138, 0.08627451, 0.2392157 ]],\n\n        ...,\n\n        [[0.3803922 , 0.3372549 , 0.5568628 ],\n         [0.47058827, 0.39607847, 0.6156863 ],\n         [0.74509805, 0.62352943, 0.854902  ],\n         ...,\n         [0.76470596, 0.627451  , 0.83921576],\n         [0.63529414, 0.47058827, 0.7176471 ],\n         [0.7137255 , 0.5176471 , 0.80392164]],\n\n        [[0.43529415, 0.34901962, 0.6313726 ],\n         [0.38431376, 0.27450982, 0.5647059 ],\n         [0.32941177, 0.18823531, 0.4784314 ],\n         ...,\n         [0.7843138 , 0.6313726 , 0.8862746 ],\n         [0.7411765 , 0.54509807, 0.8313726 ],\n         [0.7019608 , 0.47058827, 0.8000001 ]],\n\n        [[0.53333336, 0.41960788, 0.7490196 ],\n         [0.3803922 , 0.24705884, 0.58431375],\n         [0.50980395, 0.36078432, 0.69411767],\n         ...,\n         [0.47450984, 0.30980393, 0.5921569 ],\n         [0.68235296, 0.4666667 , 0.7843138 ],\n         [0.8117648 , 0.5529412 , 0.9058824 ]]],\n\n\n       [[[0.5411765 , 0.3254902 , 0.72156864],\n         [0.80392164, 0.64705884, 0.92549026],\n         [0.85098046, 0.77647066, 0.90196085],\n         ...,\n         [0.2784314 , 0.18823531, 0.5411765 ],\n         [0.25490198, 0.10196079, 0.4666667 ],\n         [0.3529412 , 0.13333334, 0.5137255 ]],\n\n        [[0.2509804 , 0.02745098, 0.43921572],\n         [0.7019608 , 0.5294118 , 0.83921576],\n         [0.9568628 , 0.85098046, 1.        ],\n         ...,\n         [0.31764707, 0.21960786, 0.5764706 ],\n         [0.3529412 , 0.19215688, 0.56078434],\n         [0.3921569 , 0.16862746, 0.54901963]],\n\n        [[0.28235295, 0.0627451 , 0.49803925],\n         [0.4666667 , 0.2784314 , 0.6156863 ],\n         [1.        , 0.8745099 , 1.        ],\n         ...,\n         [0.32156864, 0.19215688, 0.54901963],\n         [0.38431376, 0.20784315, 0.57254905],\n         [0.5176471 , 0.29411766, 0.6745098 ]],\n\n        ...,\n\n        [[0.83921576, 0.454902  , 0.7803922 ],\n         [0.93725497, 0.5647059 , 0.8862746 ],\n         [0.8588236 , 0.50980395, 0.8235295 ],\n         ...,\n         [0.54901963, 0.2509804 , 0.5019608 ],\n         [0.54901963, 0.18431373, 0.45882356],\n         [0.5921569 , 0.1764706 , 0.4666667 ]],\n\n        [[0.5568628 , 0.22352943, 0.5411765 ],\n         [0.58431375, 0.25882354, 0.57254905],\n         [0.73333335, 0.41176474, 0.73333335],\n         ...,\n         [0.7058824 , 0.3529412 , 0.6431373 ],\n         [0.6313726 , 0.22352943, 0.5176471 ],\n         [0.7176471 , 0.25882354, 0.5647059 ]],\n\n        [[0.65882355, 0.36862746, 0.6784314 ],\n         [0.7803922 , 0.48627454, 0.8078432 ],\n         [0.47450984, 0.18823531, 0.50980395],\n         ...,\n         [0.7254902 , 0.3254902 , 0.6509804 ],\n         [0.62352943, 0.18039216, 0.49411768],\n         [0.627451  , 0.13333334, 0.454902  ]]],\n\n\n       [[[0.81568635, 0.6901961 , 0.8862746 ],\n         [0.92549026, 0.77647066, 0.9607844 ],\n         [0.8862746 , 0.70980394, 0.87843144],\n         ...,\n         [0.69803923, 0.32156864, 0.7058824 ],\n         [0.8117648 , 0.39607847, 0.7803922 ],\n         [0.69803923, 0.30980393, 0.6745098 ]],\n\n        [[0.7607844 , 0.6       , 0.82745105],\n         [0.7686275 , 0.58431375, 0.8000001 ],\n         [0.86666673, 0.65882355, 0.86274517],\n         ...,\n         [0.6784314 , 0.28627452, 0.6745098 ],\n         [0.6784314 , 0.28627452, 0.6627451 ],\n         [0.6117647 , 0.25882354, 0.6117647 ]],\n\n        [[0.7254902 , 0.5019608 , 0.7803922 ],\n         [0.7490196 , 0.5137255 , 0.78823537],\n         [0.8235295 , 0.5803922 , 0.8352942 ],\n         ...,\n         [0.6509804 , 0.24313727, 0.6313726 ],\n         [0.7960785 , 0.427451  , 0.7960785 ],\n         [0.6431373 , 0.32941177, 0.67058825]],\n\n        ...,\n\n        [[0.7725491 , 0.42352945, 0.74509805],\n         [0.79215693, 0.48235297, 0.8000001 ],\n         [0.6431373 , 0.3647059 , 0.6784314 ],\n         ...,\n         [0.5764706 , 0.23137257, 0.5686275 ],\n         [0.65882355, 0.3372549 , 0.6666667 ],\n         [0.62352943, 0.32156864, 0.6431373 ]],\n\n        [[0.6627451 , 0.3372549 , 0.6431373 ],\n         [0.7490196 , 0.4666667 , 0.76470596],\n         [0.63529414, 0.38431376, 0.68235296],\n         ...,\n         [0.7372549 , 0.43529415, 0.7490196 ],\n         [0.6509804 , 0.35686275, 0.6784314 ],\n         [0.6313726 , 0.34509805, 0.6627451 ]],\n\n        [[0.8235295 , 0.5254902 , 0.81568635],\n         [0.7490196 , 0.48235297, 0.7686275 ],\n         [0.6156863 , 0.3921569 , 0.67058825],\n         ...,\n         [0.9960785 , 0.7176471 , 1.        ],\n         [0.9176471 , 0.6392157 , 0.95294124],\n         [0.77647066, 0.49803925, 0.8117648 ]]]], dtype=float32)>]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for train_x, _ in train_dataset:\n",
    "        train_step(model, train_x, optimizer)\n",
    "\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    variational_lower_bound = -loss.result()\n",
    "\n",
    "    #print(f'Epoch: {epoch}, Test set variational lower bound:\n",
    "   #        {variational_lower_bound}')\n",
    "    generate_and_save_images(model, epoch, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amfall23Copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
